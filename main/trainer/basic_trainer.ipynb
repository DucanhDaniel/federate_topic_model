{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ee166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "project_root = \"d:/MachineLearning/federated_vae\"\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from main.data.basic_dataset import BasicDataset\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from main.utils import _utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb94c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTrainer:\n",
    "    def __init__(self, \n",
    "                 model : nn.Module,\n",
    "                 dataset : BasicDataset,\n",
    "                 num_top_words = 15,\n",
    "                 epochs = 200,\n",
    "                 learning_rate = 0.002,\n",
    "                 batch_size = 200,\n",
    "                 verbose = False,\n",
    "                 device = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.num_top_words = num_top_words\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.log_interval = 1\n",
    "        self.data_size = len(self.dataset.train_data)\n",
    "        self.device = device\n",
    "\n",
    "    def make_optimizer(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "    \n",
    "    def train(self):\n",
    "        optimizer = self.make_optimizer()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_data in self.dataset.train_dataloader:\n",
    "                batch_data = batch_data.to(self.device)\n",
    "                output = self.model(batch_data)\n",
    "\n",
    "                batch_loss = output['loss']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += batch_loss * len(batch_data)\n",
    "\n",
    "            if (epoch % self.log_interval == 0):    \n",
    "                print(f\"Epoch: {epoch:03d} | Loss: {total_loss / self.data_size}\")\n",
    "\n",
    "        top_words = self.get_top_words()\n",
    "        train_theta = self.test(self.dataset.train_data)\n",
    "\n",
    "        return top_words, train_theta\n",
    "\n",
    "    def test(self, bow):\n",
    "        data_size = bow.shape[0]\n",
    "        theta = list()\n",
    "        all_idx = torch.split(torch.arange(data_size), self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for idx in all_idx:\n",
    "                batch_input = bow[idx]\n",
    "                batch_input = batch_input.to(self.device)\n",
    "                # print(batch_input.device)\n",
    "                batch_theta = self.model.get_theta(batch_input)\n",
    "                theta.extend(batch_theta.cpu().tolist())\n",
    "\n",
    "        theta = np.asarray(theta)\n",
    "        return theta\n",
    "\n",
    "    def get_beta(self):\n",
    "        beta = self.model.get_beta().detach().cpu().numpy()\n",
    "        return beta\n",
    "\n",
    "    def get_top_words(self, num_top_words=None):\n",
    "        if num_top_words is None:\n",
    "            num_top_words = self.num_top_words\n",
    "        beta = self.get_beta()\n",
    "        top_words = _utils.get_top_words(beta, self.dataset.vocab, num_top_words, self.verbose)\n",
    "        return top_words\n",
    "\n",
    "    def export_theta(self):\n",
    "        train_theta = self.test(self.dataset.train_data)\n",
    "        test_theta = self.test(self.dataset.test_data)\n",
    "        return train_theta, test_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f0eef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  11314\n",
      "test_size:  7532\n",
      "vocab_size:  5000\n",
      "average length: 110.543\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "from main.model.ETM import ETM\n",
    "test_basic_dataset = BasicDataset(\n",
    "    dataset_dir = \"../../data/20NG\"\n",
    ")\n",
    "test_model = ETM(test_basic_dataset.vocab_size).to(\"cuda\")\n",
    "test_basic_trainer = BasicTrainer(\n",
    "    model=test_model,\n",
    "    dataset=test_basic_dataset,\n",
    "    verbose=True,\n",
    "    epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf449b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 2006.024169921875\n",
      "Epoch: 001 | Loss: 1504.8192138671875\n",
      "Epoch: 002 | Loss: 1261.488525390625\n",
      "Epoch: 003 | Loss: 1135.0997314453125\n",
      "Epoch: 004 | Loss: 1056.57275390625\n",
      "Epoch: 005 | Loss: 1007.9369506835938\n",
      "Epoch: 006 | Loss: 976.2202758789062\n",
      "Epoch: 007 | Loss: 953.3737182617188\n",
      "Epoch: 008 | Loss: 936.7518920898438\n",
      "Epoch: 009 | Loss: 924.4547729492188\n",
      "Epoch: 010 | Loss: 914.7496948242188\n",
      "Epoch: 011 | Loss: 906.7327270507812\n",
      "Epoch: 012 | Loss: 900.0980224609375\n",
      "Epoch: 013 | Loss: 894.4736328125\n",
      "Epoch: 014 | Loss: 889.7579345703125\n",
      "Epoch: 015 | Loss: 885.5658569335938\n",
      "Epoch: 016 | Loss: 882.1094360351562\n",
      "Epoch: 017 | Loss: 879.121337890625\n",
      "Epoch: 018 | Loss: 876.5726318359375\n",
      "Epoch: 019 | Loss: 874.4020385742188\n",
      "Epoch: 020 | Loss: 872.5117797851562\n",
      "Epoch: 021 | Loss: 870.705078125\n",
      "Epoch: 022 | Loss: 869.1187744140625\n",
      "Epoch: 023 | Loss: 867.86328125\n",
      "Epoch: 024 | Loss: 866.7814331054688\n",
      "Epoch: 025 | Loss: 865.7118530273438\n",
      "Epoch: 026 | Loss: 864.8282470703125\n",
      "Epoch: 027 | Loss: 864.0142211914062\n",
      "Epoch: 028 | Loss: 863.3304443359375\n",
      "Epoch: 029 | Loss: 862.6168212890625\n",
      "Epoch: 030 | Loss: 862.1585693359375\n",
      "Epoch: 031 | Loss: 861.5658569335938\n",
      "Epoch: 032 | Loss: 861.0308227539062\n",
      "Epoch: 033 | Loss: 860.74462890625\n",
      "Epoch: 034 | Loss: 860.2329711914062\n",
      "Epoch: 035 | Loss: 859.8626708984375\n",
      "Epoch: 036 | Loss: 859.5189208984375\n",
      "Epoch: 037 | Loss: 859.5205078125\n",
      "Epoch: 038 | Loss: 859.1096801757812\n",
      "Epoch: 039 | Loss: 858.942626953125\n",
      "Epoch: 040 | Loss: 858.6143798828125\n",
      "Epoch: 041 | Loss: 858.4630737304688\n",
      "Epoch: 042 | Loss: 858.3217163085938\n",
      "Epoch: 043 | Loss: 858.1937255859375\n",
      "Epoch: 044 | Loss: 857.9249877929688\n",
      "Epoch: 045 | Loss: 857.5906982421875\n",
      "Epoch: 046 | Loss: 857.6051025390625\n",
      "Epoch: 047 | Loss: 857.282470703125\n",
      "Epoch: 048 | Loss: 857.2078247070312\n",
      "Epoch: 049 | Loss: 857.0534057617188\n",
      "Epoch: 050 | Loss: 857.0261840820312\n",
      "Epoch: 051 | Loss: 856.9806518554688\n",
      "Epoch: 052 | Loss: 856.6932983398438\n",
      "Epoch: 053 | Loss: 856.6549682617188\n",
      "Epoch: 054 | Loss: 856.7727661132812\n",
      "Epoch: 055 | Loss: 856.4710083007812\n",
      "Epoch: 056 | Loss: 856.3804321289062\n",
      "Epoch: 057 | Loss: 856.2952270507812\n",
      "Epoch: 058 | Loss: 856.3993530273438\n",
      "Epoch: 059 | Loss: 856.2587280273438\n",
      "Epoch: 060 | Loss: 856.2068481445312\n",
      "Epoch: 061 | Loss: 856.1278076171875\n",
      "Epoch: 062 | Loss: 856.0784912109375\n",
      "Epoch: 063 | Loss: 855.8978271484375\n",
      "Epoch: 064 | Loss: 855.8775634765625\n",
      "Epoch: 065 | Loss: 855.7921752929688\n",
      "Epoch: 066 | Loss: 855.881591796875\n",
      "Epoch: 067 | Loss: 855.7056884765625\n",
      "Epoch: 068 | Loss: 855.6209716796875\n",
      "Epoch: 069 | Loss: 855.5761108398438\n",
      "Epoch: 070 | Loss: 855.556640625\n",
      "Epoch: 071 | Loss: 855.451416015625\n",
      "Epoch: 072 | Loss: 855.427978515625\n",
      "Epoch: 073 | Loss: 855.459228515625\n",
      "Epoch: 074 | Loss: 855.335693359375\n",
      "Epoch: 075 | Loss: 855.3466796875\n",
      "Epoch: 076 | Loss: 855.2459716796875\n",
      "Epoch: 077 | Loss: 855.11669921875\n",
      "Epoch: 078 | Loss: 855.2416381835938\n",
      "Epoch: 079 | Loss: 855.1138305664062\n",
      "Epoch: 080 | Loss: 855.2359619140625\n",
      "Epoch: 081 | Loss: 855.050537109375\n",
      "Epoch: 082 | Loss: 855.0519409179688\n",
      "Epoch: 083 | Loss: 855.1041870117188\n",
      "Epoch: 084 | Loss: 855.04345703125\n",
      "Epoch: 085 | Loss: 855.020751953125\n",
      "Epoch: 086 | Loss: 854.89306640625\n",
      "Epoch: 087 | Loss: 855.0562744140625\n",
      "Epoch: 088 | Loss: 854.8493041992188\n",
      "Epoch: 089 | Loss: 854.895263671875\n",
      "Epoch: 090 | Loss: 854.8372192382812\n",
      "Epoch: 091 | Loss: 854.8084716796875\n",
      "Epoch: 092 | Loss: 854.75244140625\n",
      "Epoch: 093 | Loss: 854.8426513671875\n",
      "Epoch: 094 | Loss: 854.7390747070312\n",
      "Epoch: 095 | Loss: 854.676513671875\n",
      "Epoch: 096 | Loss: 854.543212890625\n",
      "Epoch: 097 | Loss: 854.6035766601562\n",
      "Epoch: 098 | Loss: 854.7471923828125\n",
      "Epoch: 099 | Loss: 854.5001831054688\n",
      "Topic 0: think bible looking maybe steve speed person department reason dept written hockey chicago five complete\n",
      "Topic 1: people another john quite opinions try considered red gets almost taking knows release international bought\n",
      "Topic 2: power case free newsreader turn night got black motif working religious books action road save\n",
      "Topic 3: lines organization games known understand paul package way study armenians peace hear related eric drug\n",
      "Topic 4: windows software lot public took western includes non library computing population office young images sounds\n",
      "Topic 5: subject control makes man following seems reasonable germany fonts atheism solar europe wall store firearm\n",
      "Topic 6: many two card seems check found low came care manager though secret done defense answer\n",
      "Topic 7: university year run best problems note standard local include size likely built players users personal\n",
      "Topic 8: also sure great like high call mac life book including services thinking situation des cover\n",
      "Topic 9: world program message away guess later return display cars communications sun claim sorry sell total\n",
      "Topic 10: use used research change already example price city week therefore tom command companies cable bell\n",
      "Topic 11: says window national running whole house period else value private longer congress strong posted completely\n",
      "Topic 12: know problem course important area tin east disclaimer needed neutral art cpu flames countries happening\n",
      "Topic 13: say since thing put someone means big issue james building accept deal worked environment level\n",
      "Topic 14: right usa around always getting saying text anyway state season close amendment cross univ constitution\n",
      "Topic 15: new said question made take nothing wrote money box apr live mind kind board heard\n",
      "Topic 16: nntp want internet unix sort see ftp little faith privacy uses interesting ground exist plus\n",
      "Topic 17: key government law encryption place open technology israeli full killed turkish error mode taken effect\n",
      "Topic 18: better days seem left christians told agree women ago attack dave clearly general applications worth\n",
      "Topic 19: posting please space everyone michael clipper company pay experience college anything performance happen texas business\n",
      "Topic 20: god really whether times application although sense gmt division per apple particular sometimes toronto teams\n",
      "Topic 21: way every however code anything graphics church server advance guy specific anonymous major title words\n",
      "Topic 22: get going start rights cost wanted american clear ones contact short force weapons directory present\n",
      "Topic 23: need thanks read mail trying jim fast discussion net monitor political non hold simple across\n",
      "Topic 24: just reply jesus got evidence keywords perhaps went usually league define doug owners former respect\n",
      "Topic 25: organization something see source info machine police summary general americans folks rest objective serious btw\n",
      "Topic 26: good called set day word questions pretty write school large mine fact upon society included\n",
      "Topic 27: file things etc name car thought next single jewish body today country water none banks\n",
      "Topic 28: number different enough group scsi send much security video sale done keys talk widget fbi\n",
      "Topic 29: now years last least keep ever color memory jews via news saw brian legal stop\n",
      "Topic 30: david game state wrong post president often certainly though hand christ protect act station follow\n",
      "Topic 31: max far gun truth hope address matter canada radio military laws andrew die safety asked\n",
      "Topic 32: host university others see institute bill started california argument date whatever except corporation religion much\n",
      "Topic 33: one system without using order university win side children difference build faq ibm gods federal\n",
      "Topic 34: writes work science version email let team mark cause earth section view knowledge inside figure\n",
      "Topic 35: distribution anyone come old seen next network head looks news among gordon serial food million\n",
      "Topic 36: computer available list look either want position points user product device administration laboratory community interface\n",
      "Topic 37: never still probably interested entry less common project chris designed shipping stay prevent yeah add\n",
      "Topic 38: back line help several phone just works type home similar san clinton current expect father\n",
      "Topic 39: long find inc little feel programs happened self screen fire reading groups rules evil various\n",
      "Topic 40: make organization systems tell real given christian fact ask white actually air men opinion especially\n",
      "Topic 41: information data output drive files three image engineering disk thus heard washington sent christianity son\n",
      "Topic 42: end based mean robert third light numbers dead yet keith couple living enforcement asking jon\n",
      "Topic 43: believe true using stuff states remember form response tried behind record create hit along boston\n",
      "Topic 44: like yes bad idea israel consider runs mouse bike bus baseball coming instead van claims\n",
      "Topic 45: time even first much hard support dos play everything provide war sound armenian anybody allowed\n",
      "Topic 46: article point center top mike pittsburgh statement needs service parts gas less richard count six\n",
      "Topic 47: give possible able access small love april drivers copy help theory nice ide engine arms\n",
      "Topic 48: subject part rather chip really fax simply driver nasa pub hardware input press policy try\n",
      "Topic 49: posting bit second human show original death making certain history offer smith lord doubt stupid\n"
     ]
    }
   ],
   "source": [
    "rst = test_basic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c5693725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing texts: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[19  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################### test new documents ####################################\n",
    "from main.data.preprocess import Preprocess\n",
    "\n",
    "preprocess = Preprocess()\n",
    "\n",
    "new_docs = [\n",
    "    \"This is a new document about space, including words like space, satellite, launch, orbit.\",\n",
    "    \"This is a new document about Microsoft Windows, including words like windows, files, dos.\"\n",
    "]\n",
    "\n",
    "parsed_new_docs, new_bow = preprocess.parse(new_docs, vocab=test_basic_dataset.vocab)\n",
    "print(new_bow.shape)\n",
    "\n",
    "print(new_bow.toarray())\n",
    "input = torch.as_tensor(new_bow.toarray(), device=\"cuda\").float()\n",
    "new_theta = test_basic_trainer.test(input)\n",
    "\n",
    "print(new_theta.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdab07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words, train_theta = rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "629fd223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posting please space everyone michael clipper company pay experience college anything performance happen texas business\n",
      "windows software lot public took western includes non library computing population office young images sounds\n"
     ]
    }
   ],
   "source": [
    "for x in new_theta.argmax(1):\n",
    "    print(top_words[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
