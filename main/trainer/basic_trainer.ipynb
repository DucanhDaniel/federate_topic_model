{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ee166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "project_root = \"d:/MachineLearning/federated_vae\"\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5237b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from main.data.basic_dataset import BasicDataset\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from main.utils import _utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTrainer:\n",
    "    def __init__(self, \n",
    "                 model : nn.Module,\n",
    "                 dataset : BasicDataset,\n",
    "                 num_top_words = 15,\n",
    "                 epochs = 200,\n",
    "                 learning_rate = 0.002,\n",
    "                 batch_size = 200,\n",
    "                 verbose = False,\n",
    "                 device = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.num_top_words = num_top_words\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.log_interval = 1\n",
    "        self.data_size = len(self.dataset.train_data)\n",
    "        self.device = device\n",
    "\n",
    "    def make_optimizer(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "    \n",
    "    def train(self, model_name = None, global_round = None):\n",
    "        optimizer = self.make_optimizer()\n",
    "        if model_name is not None:\n",
    "            print(f\"Client's model: {model_name} \\t | Global round: {global_round}\")\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_data in self.dataset.train_dataloader:\n",
    "                batch_data = batch_data.to(self.device)\n",
    "                output = self.model(batch_data)\n",
    "\n",
    "                batch_loss = output['loss']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += batch_loss * len(batch_data)\n",
    "\n",
    "            if (epoch % self.log_interval == 0):    \n",
    "                print(f\"Epoch: {epoch:03d} | Loss: {total_loss / self.data_size}\")\n",
    "\n",
    "        top_words = self.get_top_words()\n",
    "        train_theta = self.test(self.dataset.train_data)\n",
    "\n",
    "        return top_words, train_theta\n",
    "\n",
    "    def test(self, bow):\n",
    "        data_size = bow.shape[0]\n",
    "        theta = list()\n",
    "        all_idx = torch.split(torch.arange(data_size), self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for idx in all_idx:\n",
    "                batch_input = bow[idx]\n",
    "                batch_input = batch_input.to(self.device)\n",
    "                # print(batch_input.device)\n",
    "                batch_theta = self.model.get_theta(batch_input)\n",
    "                theta.extend(batch_theta.cpu().tolist())\n",
    "\n",
    "        theta = np.asarray(theta)\n",
    "        return theta\n",
    "\n",
    "    def get_beta(self):\n",
    "        beta = self.model.get_beta().detach().cpu().numpy()\n",
    "        return beta\n",
    "\n",
    "    def get_top_words(self, num_top_words=None):\n",
    "        if num_top_words is None:\n",
    "            num_top_words = self.num_top_words\n",
    "        beta = self.get_beta()\n",
    "        top_words = _utils.get_top_words(beta, self.dataset.vocab, num_top_words, self.verbose)\n",
    "        return top_words\n",
    "\n",
    "    def export_theta(self):\n",
    "        train_theta = self.test(self.dataset.train_data)\n",
    "        test_theta = self.test(self.dataset.test_data)\n",
    "        return train_theta, test_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0eef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  11314\n",
      "test_size:  7532\n",
      "vocab_size:  5000\n",
      "average length: 110.543\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "from main.model.ETM import ETM\n",
    "test_basic_dataset = BasicDataset(\n",
    "    dataset_dir = \"../../data/20NG\"\n",
    ")\n",
    "test_model = ETM(test_basic_dataset.vocab_size).to(\"cuda\")\n",
    "test_basic_trainer = BasicTrainer(\n",
    "    model=test_model,\n",
    "    dataset=test_basic_dataset,\n",
    "    verbose=True,\n",
    "    epochs = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf449b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 2013.8682861328125\n",
      "Epoch: 001 | Loss: 1493.218017578125\n",
      "Epoch: 002 | Loss: 1260.7904052734375\n",
      "Epoch: 003 | Loss: 1136.22802734375\n",
      "Epoch: 004 | Loss: 1062.514404296875\n",
      "Epoch: 005 | Loss: 1013.9085083007812\n",
      "Epoch: 006 | Loss: 980.9697875976562\n",
      "Epoch: 007 | Loss: 957.2120971679688\n",
      "Epoch: 008 | Loss: 940.1891479492188\n",
      "Epoch: 009 | Loss: 926.9656372070312\n",
      "Epoch: 010 | Loss: 916.5875244140625\n",
      "Epoch: 011 | Loss: 908.2130126953125\n",
      "Epoch: 012 | Loss: 901.1885375976562\n",
      "Epoch: 013 | Loss: 895.0530395507812\n",
      "Epoch: 014 | Loss: 890.1278076171875\n",
      "Epoch: 015 | Loss: 885.719482421875\n",
      "Epoch: 016 | Loss: 882.3067016601562\n",
      "Epoch: 017 | Loss: 879.1509399414062\n",
      "Epoch: 018 | Loss: 876.6295776367188\n",
      "Epoch: 019 | Loss: 874.5859375\n",
      "Epoch: 020 | Loss: 872.5487670898438\n",
      "Epoch: 021 | Loss: 870.796875\n",
      "Epoch: 022 | Loss: 869.3773803710938\n",
      "Epoch: 023 | Loss: 867.9207763671875\n",
      "Epoch: 024 | Loss: 866.8463134765625\n",
      "Epoch: 025 | Loss: 865.6793212890625\n",
      "Epoch: 026 | Loss: 864.95458984375\n",
      "Epoch: 027 | Loss: 864.2662353515625\n",
      "Epoch: 028 | Loss: 863.345703125\n",
      "Epoch: 029 | Loss: 862.7256469726562\n",
      "Epoch: 030 | Loss: 862.0816650390625\n",
      "Epoch: 031 | Loss: 861.5335083007812\n",
      "Epoch: 032 | Loss: 861.1895751953125\n",
      "Epoch: 033 | Loss: 860.7037963867188\n",
      "Epoch: 034 | Loss: 860.1552124023438\n",
      "Epoch: 035 | Loss: 859.84716796875\n",
      "Epoch: 036 | Loss: 859.5130615234375\n",
      "Epoch: 037 | Loss: 859.211181640625\n",
      "Epoch: 038 | Loss: 859.0623168945312\n",
      "Epoch: 039 | Loss: 858.8090209960938\n",
      "Epoch: 040 | Loss: 858.5130615234375\n",
      "Epoch: 041 | Loss: 858.3618774414062\n",
      "Epoch: 042 | Loss: 858.1427612304688\n",
      "Epoch: 043 | Loss: 857.9765014648438\n",
      "Epoch: 044 | Loss: 857.8347778320312\n",
      "Epoch: 045 | Loss: 857.7372436523438\n",
      "Epoch: 046 | Loss: 857.4876098632812\n",
      "Epoch: 047 | Loss: 857.3587646484375\n",
      "Epoch: 048 | Loss: 857.298583984375\n",
      "Epoch: 049 | Loss: 857.0708618164062\n",
      "Epoch: 050 | Loss: 857.0980834960938\n",
      "Epoch: 051 | Loss: 857.0136108398438\n",
      "Epoch: 052 | Loss: 856.822265625\n",
      "Epoch: 053 | Loss: 856.7191772460938\n",
      "Epoch: 054 | Loss: 856.6398315429688\n",
      "Epoch: 055 | Loss: 856.3988037109375\n",
      "Epoch: 056 | Loss: 856.5140380859375\n",
      "Epoch: 057 | Loss: 856.401123046875\n",
      "Epoch: 058 | Loss: 856.3255004882812\n",
      "Epoch: 059 | Loss: 856.4198608398438\n",
      "Epoch: 060 | Loss: 856.2188720703125\n",
      "Epoch: 061 | Loss: 856.1542358398438\n",
      "Epoch: 062 | Loss: 856.0581665039062\n",
      "Epoch: 063 | Loss: 856.0942993164062\n",
      "Epoch: 064 | Loss: 855.998291015625\n",
      "Epoch: 065 | Loss: 855.9566650390625\n",
      "Epoch: 066 | Loss: 855.84619140625\n",
      "Epoch: 067 | Loss: 855.91357421875\n",
      "Epoch: 068 | Loss: 855.8551635742188\n",
      "Epoch: 069 | Loss: 855.7582397460938\n",
      "Epoch: 070 | Loss: 855.7783203125\n",
      "Epoch: 071 | Loss: 855.6011962890625\n",
      "Epoch: 072 | Loss: 855.6287841796875\n",
      "Epoch: 073 | Loss: 855.7766723632812\n",
      "Epoch: 074 | Loss: 855.6810913085938\n",
      "Epoch: 075 | Loss: 855.6365356445312\n",
      "Epoch: 076 | Loss: 855.5451049804688\n",
      "Epoch: 077 | Loss: 855.5199584960938\n",
      "Epoch: 078 | Loss: 855.4132080078125\n",
      "Epoch: 079 | Loss: 855.4050903320312\n",
      "Epoch: 080 | Loss: 855.27197265625\n",
      "Epoch: 081 | Loss: 855.3438110351562\n",
      "Epoch: 082 | Loss: 855.4002075195312\n",
      "Epoch: 083 | Loss: 855.3814697265625\n",
      "Epoch: 084 | Loss: 855.33056640625\n",
      "Epoch: 085 | Loss: 855.2317504882812\n",
      "Epoch: 086 | Loss: 855.229736328125\n",
      "Epoch: 087 | Loss: 855.1218872070312\n",
      "Epoch: 088 | Loss: 855.195068359375\n",
      "Epoch: 089 | Loss: 855.2763671875\n",
      "Epoch: 090 | Loss: 855.1913452148438\n",
      "Epoch: 091 | Loss: 855.0783081054688\n",
      "Epoch: 092 | Loss: 855.0364990234375\n",
      "Epoch: 093 | Loss: 855.0776977539062\n",
      "Epoch: 094 | Loss: 855.14697265625\n",
      "Epoch: 095 | Loss: 854.9780883789062\n",
      "Epoch: 096 | Loss: 854.995849609375\n",
      "Epoch: 097 | Loss: 854.9676513671875\n",
      "Epoch: 098 | Loss: 855.0469360351562\n",
      "Epoch: 099 | Loss: 855.0411987304688\n",
      "Topic 0: people god true fact says end small exactly rest took firearms keyboard rom inside regular\n",
      "Topic 1: never example window news getting writes big although armenian single company israeli guns mine difference\n",
      "Topic 2: computer writes set team clipper motif numbers let either put bob cases keith simply present\n",
      "Topic 3: question life public inc encryption area claim light sun provide advance ftp error univ groups\n",
      "Topic 4: long place state note often matter everyone sense agree deal words sometimes free saw military\n",
      "Topic 5: help someone try problems scsi change disk games pittsburgh hockey road federal support keys boston\n",
      "Topic 6: around hard far security original sorry mode personal early response morality market goal required air\n",
      "Topic 7: use going using find believe internet others men food via defense asked digital late statement\n",
      "Topic 8: since least without chip box days consider christ ground different within contact look expect political\n",
      "Topic 9: know thanks files robert price jim drivers christians still mouse gordon thinking times knows lots\n",
      "Topic 10: bit version software told color everything san today interested war upon disclaimer individual finally average\n",
      "Topic 11: said back sure high jesus thought control left white output body night build product nhl\n",
      "Topic 12: point drive old technology home little kind board care check uses hardware killed time word\n",
      "Topic 13: government house address means graphics side money usually started fire happened act secret family advice\n",
      "Topic 14: two now thing data car used already general evidence center bike germany built quality whole\n",
      "Topic 15: good usa please server bible enough fast dept later anybody taking looks normal summary excellent\n",
      "Topic 16: many number right found person ever send apr open anything cause anti exist pay written\n",
      "Topic 17: organization period theory weeks rules laboratory originator self virginia switch significant motherboard shots summer somewhat\n",
      "Topic 18: case keep order else department top wanted dod cards effect plus player complete land devices\n",
      "Topic 19: writes better available local mac certainly love health experience design msg places gary gif happy\n",
      "Topic 20: got still instead correct guess brian guy service corporation ones involved americans easily congress magazine\n",
      "Topic 21: subject john second phone group right pub privacy project worse ontario responses boulder vlb transmission\n",
      "Topic 22: lines now yet clinton half algorithm lost bbs escrow stated charles stephen option table door\n",
      "Topic 23: problem bill heard display view tape situation months population medical format million tin word administration\n",
      "Topic 24: just things want like file done little happen per model right teams insurance included applications\n",
      "Topic 25: make mail run tell list three answer given standard speed show pretty put short black\n",
      "Topic 26: think see every systems away christian important national seem points press peter talk force obvious\n",
      "Topic 27: university mind front legal members peace driving nazi missing pens roads pope changes reaction tough\n",
      "Topic 28: lot get power best actually ask hand running code making different cost programs unix season\n",
      "Topic 29: work distribution last line etc let sound particular hit jews interesting thanks international newsgroup reading\n",
      "Topic 30: one information key rather wrong possible etc taken ago california four common cover knowledge looked\n",
      "Topic 31: really first called call source trying research hope private anything dave low easy story report\n",
      "Topic 32: also following yes mean came several institute real states another faith office media police package\n",
      "Topic 33: anyone system day based next part access memory saying services religion opinion jewish rate save\n",
      "Topic 34: even way always man driver laws nasa smith oil useful los decided across lack internal\n",
      "Topic 35: much years just read however science non looking play current known mike faq nice israel\n",
      "Topic 36: great space nntp american whether full video likely kill existence directory tom die language engineering\n",
      "Topic 37: windows course human argument perhaps james especially times wrote crime weapons value offer college moral\n",
      "Topic 38: take less works canada application fine radio including chicago anonymous discussion needed views hold fall\n",
      "Topic 39: like post dos washington almost start talking together else friends shot title total chris tools\n",
      "Topic 40: article nntp david probably keywords state steve appreciated serial fbi gmt mean articles court prove\n",
      "Topic 41: need reply message either fax buy country players users text baseball banks currently action main\n",
      "Topic 42: new game give seems program michael since stuff info user free screen manager form allow\n",
      "Topic 43: time able mark went church april win truth makes division images limited parts wire christianity\n",
      "Topic 44: max host used bad image book remember death anyway working earth clear performance move otherwise\n",
      "Topic 45: get say card gun support though armenians understand seen include become type drives policy date\n",
      "Topic 46: posting want email reason feel rights issue sale comes machine paul face network women gods\n",
      "Topic 47: world article come made another couple bus goes stop league results son basic safety meaning\n",
      "Topic 48: nothing name law maybe opinions large school turkish write tried society newsreader position head reference\n",
      "Topic 49: something first year look questions idea net simple size western dead longer water behind near\n"
     ]
    }
   ],
   "source": [
    "rst = test_basic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5693725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing texts: 100%|██████████| 2/2 [00:00<00:00, 1947.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[36 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################### test new documents ####################################\n",
    "from main.data.preprocess import Preprocess\n",
    "\n",
    "preprocess = Preprocess()\n",
    "\n",
    "new_docs = [\n",
    "    \"This is a new document about space, including words like space, satellite, launch, orbit.\",\n",
    "    \"This is a new document about Microsoft Windows, including words like windows, files, dos.\"\n",
    "]\n",
    "\n",
    "parsed_new_docs, new_bow = preprocess.parse(new_docs, vocab=test_basic_dataset.vocab)\n",
    "print(new_bow.shape)\n",
    "\n",
    "print(new_bow.toarray())\n",
    "input = torch.as_tensor(new_bow.toarray(), device=\"cuda\").float()\n",
    "new_theta = test_basic_trainer.test(input)\n",
    "\n",
    "print(new_theta.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdab07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words, train_theta = rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f680114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main.evaluation.topic_coherence as tc\n",
    "import main.evaluation.topic_diversity as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25c958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic coherence: 0.22844494565450063\n",
      "Topic diversity: 0.956\n"
     ]
    }
   ],
   "source": [
    "coherence_score = tc._coherence(test_basic_dataset.train_texts, test_basic_dataset.vocab, top_words)\n",
    "diversity_score = td._diversity(top_words)\n",
    "\n",
    "print(f\"Topic coherence: {coherence_score}\")\n",
    "print(f\"Topic diversity: {diversity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629fd223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great space nntp american whether full video likely kill existence directory tom die language engineering\n",
      "like post dos washington almost start talking together else friends shot title total chris tools\n"
     ]
    }
   ],
   "source": [
    "for x in new_theta.argmax(1):\n",
    "    print(top_words[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
