{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data.basic_dataset import BasicDataset\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils import _utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb94c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTrainer:\n",
    "    def __init__(self, \n",
    "                 model : nn.Module,\n",
    "                 dataset : BasicDataset,\n",
    "                 num_top_words = 15,\n",
    "                 epochs = 200,\n",
    "                 learning_rate = 0.002,\n",
    "                 batch_size = 200,\n",
    "                 verbose = False,\n",
    "                 device = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.num_top_words = num_top_words\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.log_interval = 1\n",
    "        self.data_size = len(self.dataset.train_data)\n",
    "        self.device = device\n",
    "\n",
    "    def make_optimizer(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "    \n",
    "    def train(self, model_name = None, global_round = None):\n",
    "        optimizer = self.make_optimizer()\n",
    "        if model_name is not None:\n",
    "            print(f\"Client's model: {model_name} \\t | Global round: {global_round}\")\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_data in self.dataset.train_dataloader:\n",
    "                batch_data = batch_data.to(self.device)\n",
    "                output = self.model(batch_data)\n",
    "\n",
    "                batch_loss = output['loss']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += batch_loss * len(batch_data)\n",
    "\n",
    "            if (epoch % self.log_interval == 0):    \n",
    "                print(f\"Epoch: {epoch:03d} | Loss: {total_loss / self.data_size}\")\n",
    "\n",
    "        top_words = self.get_top_words()\n",
    "        train_theta = self.test(self.dataset.train_data)\n",
    "\n",
    "        return top_words, train_theta\n",
    "\n",
    "    def test(self, bow):\n",
    "        data_size = bow.shape[0]\n",
    "        theta = list()\n",
    "        all_idx = torch.split(torch.arange(data_size), self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for idx in all_idx:\n",
    "                batch_input = bow[idx]\n",
    "                batch_input = batch_input.to(self.device)\n",
    "                # print(batch_input.device)\n",
    "                batch_theta = self.model.get_theta(batch_input)\n",
    "                theta.extend(batch_theta.cpu().tolist())\n",
    "\n",
    "        theta = np.asarray(theta)\n",
    "        return theta\n",
    "\n",
    "    def get_beta(self):\n",
    "        beta = self.model.get_beta().detach().cpu().numpy()\n",
    "        return beta\n",
    "\n",
    "    def get_top_words(self, num_top_words=None):\n",
    "        if num_top_words is None:\n",
    "            num_top_words = self.num_top_words\n",
    "        beta = self.get_beta()\n",
    "        top_words = _utils.get_top_words(beta, self.dataset.vocab, num_top_words, self.verbose)\n",
    "        return top_words\n",
    "\n",
    "    def export_theta(self):\n",
    "        train_theta = self.test(self.dataset.train_data)\n",
    "        test_theta = self.test(self.dataset.test_data)\n",
    "        return train_theta, test_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0eef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  11314\n",
      "test_size:  7532\n",
      "vocab_size:  5000\n",
      "average length: 110.543\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "from model.ETM import ETM\n",
    "test_basic_dataset = BasicDataset(\n",
    "    dataset_dir = \"../data/20NG\"\n",
    ")\n",
    "test_model = ETM(test_basic_dataset.vocab_size).to(\"cuda\")\n",
    "test_basic_trainer = BasicTrainer(\n",
    "    model=test_model,\n",
    "    dataset=test_basic_dataset,\n",
    "    verbose=True,\n",
    "    epochs = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf449b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 2062.722900390625\n",
      "Epoch: 001 | Loss: 1542.28955078125\n",
      "Epoch: 002 | Loss: 1282.6055908203125\n",
      "Epoch: 003 | Loss: 1137.4420166015625\n",
      "Epoch: 004 | Loss: 1055.7479248046875\n",
      "Epoch: 005 | Loss: 1007.5901489257812\n",
      "Epoch: 006 | Loss: 976.6014404296875\n",
      "Epoch: 007 | Loss: 955.3668212890625\n",
      "Epoch: 008 | Loss: 939.692138671875\n",
      "Epoch: 009 | Loss: 927.1049194335938\n",
      "Epoch: 010 | Loss: 916.8370971679688\n",
      "Epoch: 011 | Loss: 908.0200805664062\n",
      "Epoch: 012 | Loss: 901.1065673828125\n",
      "Epoch: 013 | Loss: 895.1832885742188\n",
      "Epoch: 014 | Loss: 890.0108642578125\n",
      "Epoch: 015 | Loss: 885.7984619140625\n",
      "Epoch: 016 | Loss: 882.3370971679688\n",
      "Epoch: 017 | Loss: 879.3961181640625\n",
      "Epoch: 018 | Loss: 876.7074584960938\n",
      "Epoch: 019 | Loss: 874.4373168945312\n",
      "Epoch: 020 | Loss: 872.562744140625\n",
      "Epoch: 021 | Loss: 870.8250122070312\n",
      "Epoch: 022 | Loss: 869.222900390625\n",
      "Epoch: 023 | Loss: 867.946533203125\n",
      "Epoch: 024 | Loss: 866.9002075195312\n",
      "Epoch: 025 | Loss: 865.8560180664062\n",
      "Epoch: 026 | Loss: 865.0390625\n",
      "Epoch: 027 | Loss: 864.2393188476562\n",
      "Epoch: 028 | Loss: 863.585205078125\n",
      "Epoch: 029 | Loss: 862.8993530273438\n",
      "Topic 0: got thing john course true machine making request language chicago frank works sites lord former\n",
      "Topic 1: going best start sale box side understand major bob cars total toronto ibm million gas\n",
      "Topic 2: power though chip info given change armenians states agree unless black self deal couple dave\n",
      "Topic 3: never take fact whether standard color encryption likely uses faq big allow applications claims sources\n",
      "Topic 4: last run version ever open dept working care points posted via city offer hit defense\n",
      "Topic 5: lines organization time world email mark others took york fire league food gordon solution supply\n",
      "Topic 6: say year better window group today call graphics went steve except congress week media completely\n",
      "Topic 7: just also anyone end lot second let note place next always speed newsreader exactly advance\n",
      "Topic 8: university someone law access keywords mac keep paul christians political strong gods series results fan\n",
      "Topic 9: posting see find called bit yes seem idea output love services sometimes includes machines values\n",
      "Topic 10: get play certainly anti reading parts none programs player complete criminals operation majority appears traffic\n",
      "Topic 11: years disk christian trying drivers put test model specific runs federal sun thank colorado meaning\n",
      "Topic 12: now jesus control clipper view later package word months family atheists near court digital result\n",
      "Topic 13: made hard technology left makes house killed everything israeli clinton fbi water policy longer average\n",
      "Topic 14: please writes looking using reason memory user heard california guns special bus leave takes disclaimer\n",
      "Topic 15: host government rather fax send men unix write monitor talk members hear behind manager air\n",
      "Topic 16: know program old rights certain put top market tax whatever matthew included georgia australia modern\n",
      "Topic 17: people first every wrote anybody institute western greek willing outside existence usenet eric letter bought\n",
      "Topic 18: use data set different files using buy canada size single everyone guess display radio various\n",
      "Topic 19: man less message armenian ones country check away statement dod accept doubt thomas cover created\n",
      "Topic 20: know mail probably yet already users college hockey started words personal event works easily sell\n",
      "Topic 21: nntp really since systems problems nothing either source means pretty players press nasa face front\n",
      "Topic 22: things etc least order gun small dos mike truth server season three gmt net young\n",
      "Topic 23: subject said turkish evidence show low military contact turks away effect serious job directly bios\n",
      "Topic 24: used line current michael able crime involved land international wait faster msg gif german null\n",
      "Topic 25: like thanks children general application newsgroup places somewhere definition mhz causes examples social columbia weak\n",
      "Topic 26: good space another case make give internet else perhaps interested similar fine happen act united\n",
      "Topic 27: windows question great following service head sorry sort tin worth built pittsburgh said asking safety\n",
      "Topic 28: subject way help key make high game local include live light saw build controller numbers\n",
      "Topic 29: car inc whole news bad clear office often division univ scott driver theory ago trade\n",
      "Topic 30: system work national available death found easy remember cut church performance doug originator thus archive\n",
      "Topic 31: two around actually games area type earth short christ came return richard form guy necessarily\n",
      "Topic 32: possible seen team maybe answer large win written discussion figure add report expect news laboratory\n",
      "Topic 33: distribution study kind opinion instead body weeks police error text ram floppy enforcement month logic\n",
      "Topic 34: new reply want right number david enough free simply president feel san turn entry privacy\n",
      "Topic 35: file card done important consider stop happened summary comes value particular peter cause main wants\n",
      "Topic 36: computer usa science claim known correct screen level mine ground books speak son rom author\n",
      "Topic 37: max seems keys especially rate child commercial release received belief field stay aware bmw lunar\n",
      "Topic 38: god come opinions days times american jim faith road story secret inside therefore follow action\n",
      "Topic 39: however software wrong provide communications history board robert bible copy experience engineering computing reasonable administration\n",
      "Topic 40: even sure far says support quite questions without human book tried washington past third weapons\n",
      "Topic 41: many drive state little try post white school hope anonymous several talking running stuff religious\n",
      "Topic 42: one much problem information anything image network considered close kill ideas class christianity insurance society\n",
      "Topic 43: look public jews based home israel issue company anyone video hardware fast matter department position\n",
      "Topic 44: article name day thought code april four anyway pay address plus supposed designed option stephen\n",
      "Topic 45: back part point real mean life list person bill without wanted period mode health including\n",
      "Topic 46: long getting war told apr almost price business reported tom explain needs secure choice closed\n",
      "Topic 47: need still believe tell center scsi phone original argument religion asked early several mentioned keith\n",
      "Topic 48: example full ask mind three james sense cost reference images america room conference base hall\n",
      "Topic 49: writes think something read research non money hand sound voice widget save issues final goal\n"
     ]
    }
   ],
   "source": [
    "rst = test_basic_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5693725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing texts: 100%|██████████| 2/2 [00:00<00:00, 329.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[26 27]\n"
     ]
    }
   ],
   "source": [
    "########################### test new documents ####################################\n",
    "from data.preprocess import Preprocess\n",
    "\n",
    "preprocess = Preprocess()\n",
    "\n",
    "new_docs = [\n",
    "    \"This is a new document about space, including words like space, satellite, launch, orbit.\",\n",
    "    \"This is a new document about Microsoft Windows, including words like windows, files, dos.\"\n",
    "]\n",
    "\n",
    "parsed_new_docs, new_bow = preprocess.parse(new_docs, vocab=test_basic_dataset.vocab)\n",
    "print(new_bow.shape)\n",
    "\n",
    "print(new_bow.toarray())\n",
    "input = torch.as_tensor(new_bow.toarray(), device=\"cuda\").float()\n",
    "new_theta = test_basic_trainer.test(input)\n",
    "\n",
    "print(new_theta.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdab07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words, train_theta = rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f680114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation.topic_coherence as tc\n",
    "import evaluation.topic_diversity as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_score = tc._coherence(test_basic_dataset.train_texts, test_basic_dataset.vocab, top_words)\n",
    "diversity_score = td._diversity(top_words)\n",
    "\n",
    "print(f\"Topic coherence: {coherence_score}\")\n",
    "print(f\"Topic diversity: {diversity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fd223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great space nntp american whether full video likely kill existence directory tom die language engineering\n",
      "like post dos washington almost start talking together else friends shot title total chris tools\n"
     ]
    }
   ],
   "source": [
    "for x in new_theta.argmax(1):\n",
    "    print(top_words[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
