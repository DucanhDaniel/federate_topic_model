{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720f1a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\MachineLearning\\federated_vae\\main\n"
     ]
    }
   ],
   "source": [
    "%cd ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1964b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Flower 1.19.0 / PyTorch 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c028d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:  11314\n",
      "test_size:  7532\n",
      "vocab_size:  5000\n",
      "average length: 110.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9738.00it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 11789.37it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8739.26it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10508.26it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8431.08it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10921.46it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9202.96it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 11279.57it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9010.44it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 11033.99it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8500.85it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10163.67it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 7151.77it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 9099.57it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8559.06it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10252.32it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8318.79it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10278.02it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9142.18it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10839.09it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8193.08it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 9770.16it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8843.76it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10763.17it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8072.59it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 9485.27it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9644.47it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 9382.97it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8724.27it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10843.10it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9910.01it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 11964.06it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8960.87it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10440.72it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 8909.09it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10819.98it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 6566.95it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 7548.18it/s]\n",
      "loading train texts: 100%|██████████| 565/565 [00:00<00:00, 9181.32it/s]\n",
      "parsing texts: 100%|██████████| 565/565 [00:00<00:00, 10925.54it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 2\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "from test_flwr import get_all_vocab, split_data\n",
    "vocab = get_all_vocab([\"../data/20NG\"])\n",
    "datasets = split_data(dir = \"../data/20NG\", num_split=20, vocab = vocab, batch_size= BATCH_SIZE)[:NUM_CLIENTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "805637fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ETM import ETM\n",
    "from trainer.basic_trainer import BasicTrainer\n",
    "\n",
    "# net = ETM(len(vocab)).to(DEVICE)\n",
    "\n",
    "# trainer = BasicTrainer(model = net, dataset = datasets[0], epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "#                        log_interval=10)\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e79e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = trainer.get_top_words()\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caebed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa431d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.basic_dataset import RawDataset\n",
    "class FlowerClient(NumPyClient):\n",
    "  def __init__(self, net, dataset : RawDataset):\n",
    "    self.net = net\n",
    "    self.dataset = dataset\n",
    "    self.trainer = BasicTrainer(net, dataset, epochs = 1, log_interval=10, device = DEVICE)\n",
    "\n",
    "  # return the current local model parameters\n",
    "  def get_parameters(self, config):\n",
    "    return get_parameters(self.net)\n",
    "\n",
    "  # receive global parameter, train, return updated model to server\n",
    "  def fit(self, parameters, config):\n",
    "    set_parameters(self.net, parameters)\n",
    "    self.trainer.train()\n",
    "\n",
    "    return get_parameters(self.net), len(self.dataset.train_texts), {}\n",
    "\n",
    "  # receive global parameter, evaluate model from local's data, return the evaluation result\n",
    "  def evaluate(self, parameters, config):\n",
    "    set_parameters(self.net, parameters)\n",
    "    loss, acc = -1, -1\n",
    "    return float(loss), 0, {\"accuracy\":float(acc)}\n",
    "  \n",
    "\n",
    "\n",
    "test = FlowerClient(ETM(len(vocab)), datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8236b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    net = ETM(len(vocab)).to(DEVICE)\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data partition\n",
    "    # Read the node_config to fetch data partition associated to this node\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    dataset = datasets[partition_id]\n",
    "\n",
    "    # Create a single Flower client representing a single organization\n",
    "    # FlowerClient is a subclass of NumPyClient, so we need to call .to_client()\n",
    "    # to convert it to a subclass of `flwr.client.Client`\n",
    "    return FlowerClient(net, dataset).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55018fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of ETM(\n",
      "  (encoder1): Sequential(\n",
      "    (0): Linear(in_features=5000, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (fc21): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc22): Linear(in_features=800, out_features=50, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(test.net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233689d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    \"\"\"Construct components that set the ServerApp behaviour.\n",
    "\n",
    "    You can use the settings in `context.run_config` to parameterize the\n",
    "    construction of all elements (e.g the strategy or the number of rounds)\n",
    "    wrapped in the returned ServerAppComponents object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure the server for 5 rounds of training\n",
    "    config = ServerConfig(num_rounds=5)\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=NUM_CLIENTS,\n",
    "        min_available_clients=NUM_CLIENTS,\n",
    "    )\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a830fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fadb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94mDEBUG 2025-07-12 22:48:56,926\u001b[0m:     Asyncio event loop already running.\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,928\u001b[0m:     Logger propagate set to False\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,929\u001b[0m:     Pre-registering run with id 2335252092985170524\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,933\u001b[0m:     Using InMemoryState\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,934\u001b[0m:     Using InMemoryState\n",
      "\u001b[92mINFO 2025-07-12 22:48:56,937\u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,939\u001b[0m:     Using InMemoryState\n",
      "\u001b[92mINFO 2025-07-12 22:48:56,940\u001b[0m:      \n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,941\u001b[0m:     Registered 2 nodes\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,942\u001b[0m:     Supported backends: ['ray']\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,943\u001b[0m:     Initialising: RayBackend\n",
      "\u001b[92mINFO 2025-07-12 22:48:56,943\u001b[0m:      [INIT]\n",
      "\u001b[94mDEBUG 2025-07-12 22:48:56,943\u001b[0m:     Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 1.0}, 'init_args': {}, 'actor': {'tensorflow': 0}}\n",
      "\u001b[92mINFO 2025-07-12 22:48:56,944\u001b[0m:      Requesting initial parameters from one random client\n",
      "2025-07-12 22:49:00,401\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:02,220\u001b[0m:     Constructed ActorPool with: 1 actors\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:02,221\u001b[0m:     Using InMemoryState\n",
      "\u001b[92mINFO 2025-07-12 22:49:19,465\u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO 2025-07-12 22:49:19,466\u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO 2025-07-12 22:49:19,467\u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO 2025-07-12 22:49:19,467\u001b[0m:      \n",
      "\u001b[92mINFO 2025-07-12 22:49:19,467\u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO 2025-07-12 22:49:19,468\u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22140)\u001b[0m Epoch: 000 | Loss: 1960.4237060546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2025-07-12 22:49:20,698\u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[93mWARNING 2025-07-12 22:49:20,774\u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO 2025-07-12 22:49:20,778\u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22140)\u001b[0m Epoch: 000 | Loss: 2313.81982421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2025-07-12 22:49:21,675\u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
      "\u001b[91mERROR 2025-07-12 22:49:21,677\u001b[0m:     ServerApp thread raised an exception: float division by zero\n",
      "\u001b[91mERROR 2025-07-12 22:49:21,819\u001b[0m:     Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\simulation\\run_simulation.py\", line 268, in server_th_with_start_checks\n",
      "    updated_context = _run(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\run_serverapp.py\", line 62, in run\n",
      "    server_app(grid=grid, context=context)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server_app.py\", line 166, in __call__\n",
      "    start_grid(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\compat\\app.py\", line 90, in start_grid\n",
      "    hist = run_fl(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server.py\", line 492, in run_fl\n",
      "    hist, elapsed_time = server.fit(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server.py\", line 145, in fit\n",
      "    res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server.py\", line 203, in evaluate_round\n",
      "    ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\strategy\\fedavg.py\", line 270, in aggregate_evaluate\n",
      "    loss_aggregated = weighted_loss_avg(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\strategy\\aggregate.py\", line 221, in weighted_loss_avg\n",
      "    return sum(weighted_losses) / num_total_evaluation_examples\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:21,820\u001b[0m:     ServerApp finished running.\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:21,820\u001b[0m:     Triggered stop event for Simulation Engine.\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\simulation\\run_simulation.py\", line 268, in server_th_with_start_checks\n",
      "    updated_context = _run(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\run_serverapp.py\", line 62, in run\n",
      "    server_app(grid=grid, context=context)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server_app.py\", line 166, in __call__\n",
      "    start_grid(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\compat\\app.py\", line 90, in start_grid\n",
      "    hist = run_fl(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server.py\", line 492, in run_fl\n",
      "    hist, elapsed_time = server.fit(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server.py\", line 145, in fit\n",
      "    res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\server.py\", line 203, in evaluate_round\n",
      "    ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\strategy\\fedavg.py\", line 270, in aggregate_evaluate\n",
      "    loss_aggregated = weighted_loss_avg(\n",
      "  File \"d:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\server\\strategy\\aggregate.py\", line 221, in weighted_loss_avg\n",
      "    return sum(weighted_losses) / num_total_evaluation_examples\n",
      "ZeroDivisionError: float division by zero\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:22,638\u001b[0m:     Terminated 1 actors\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:23,582\u001b[0m:     Terminated RayBackend\n",
      "\u001b[94mDEBUG 2025-07-12 22:49:26,598\u001b[0m:     Queue timeout. No context received.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception in ServerApp thread",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run simulation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_supernodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLIENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\simulation\\run_simulation.py:211\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(server_app, client_app, num_supernodes, backend_name, backend_config, enable_tf_gpu_growth, verbose_logging)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_tf_gpu_growth:\n\u001b[0;32m    203\u001b[0m     warn_deprecated_feature_with_example(\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `enable_tf_gpu_growth=True` is deprecated.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    205\u001b[0m         example_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead, set the `TF_FORCE_GPU_ALLOW_GROWTH` environment \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mflwr.simulation.run_simulationt(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 211\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43m_run_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_supernodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_supernodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_tf_gpu_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_tf_gpu_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexit_event\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEventType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPYTHON_API_RUN_SIMULATION_LEAVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\simulation\\run_simulation.py:510\u001b[0m, in \u001b[0;36m_run_simulation\u001b[1;34m(num_supernodes, exit_event, client_app, server_app, backend_name, backend_config, client_app_attr, server_app_attr, server_app_run_config, app_dir, flwr_dir, run, enable_tf_gpu_growth, verbose_logging, is_app)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m asyncio_loop_running:\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;66;03m# Set logger propagation to False to prevent duplicated log output in Colab.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m         logger \u001b[38;5;241m=\u001b[39m set_logger_propagation(logger, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 510\u001b[0m     updated_context \u001b[38;5;241m=\u001b[39m \u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_context\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\TMenv\\lib\\site-packages\\flwr\\simulation\\run_simulation.py:408\u001b[0m, in \u001b[0;36m_main_loop\u001b[1;34m(num_supernodes, backend_name, backend_config_stream, app_dir, is_app, enable_tf_gpu_growth, run, exit_event, flwr_dir, client_app, client_app_attr, server_app, server_app_attr, server_app_run_config)\u001b[0m\n\u001b[0;32m    406\u001b[0m         serverapp_th\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m server_app_thread_has_exception\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m--> 408\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException in ServerApp thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    410\u001b[0m log(DEBUG, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopping Simulation Engine now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_context\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception in ServerApp thread"
     ]
    }
   ],
   "source": [
    "# Run simulation\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    "    verbose_logging=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
